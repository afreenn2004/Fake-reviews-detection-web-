# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nnd4m_jFcXzAOlqtGb5vaM-W5BwL77mu

    
import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model, scaler, vectorizer
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return scaler, vectorizer, metadata_model

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str) and text.strip():
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stop_words)
        features['sentiment'] = sia.polarity_scores(text)['compound']
    else:
        features = {key: 0 for key in [
            'review_length', 'uppercase_word_count', 'exclamation_count', 'question_count',
            'punctuation_count', 'avg_word_length', 'flesch_reading_ease', 'stopword_count', 'sentiment'
        ]}
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.cpu().numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        # x is a list of texts
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

def prepare_metadata_input(metadata_features, vectorizer, text):
    # Vectorize text (must be a list)
    text_vector = vectorizer.transform([text]).toarray()
    # Metadata features as array (shape: 1,9)
    metadata_array = np.array([[metadata_features[col] for col in sorted(metadata_features.keys())]])
    # Combine vectorized text and metadata features horizontally
    combined = np.hstack([text_vector, metadata_array])
    return combined, text_vector, metadata_array

# Main app
def main():


    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, vectorizer = load_metadata_model()

    if "reset" not in st.session_state:
        st.session_state.reset = False
    if st.session_state.reset:
        st.session_state.reset = False
        st.experimental_rerun()    

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
            return
        with st.spinner("Analyzing review and generating explanations..."):
            # BERT Prediction
            bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
            label = "üî¥ Fake Review " if bert_pred == 1 else "üü¢ Genuine Review "
            st.subheader(f"Prediction: {label}")

            features = extract_metadata_features(user_review)
            combined_input, text_vector, metadata_array = prepare_metadata_input(
                features, vectorizer, user_review
            )
            scaled_features = scaler.transform(combined_input)
            meta_probs = metadata_model.predict_proba(scaled_features)

            bert_prob_genuine = bert_probs[1]
            meta_prob_genuine = meta_probs[0][1]
            ensemble_prob_genuine = (bert_prob_genuine + meta_prob_genuine) / 2.0
            ensemble_pred = 1 if ensemble_prob_genuine > 0.5 else 0
            ensemble_percent = round(ensemble_prob_genuine * 100, 2)
            st.markdown(f"**üí° Confidence Behind the Answer:** {ensemble_percent}%")

            # Visual progress bar
            st.progress(int(ensemble_percent))

            if ensemble_percent >= 70:
                st.success("üåü Highly Confident Prediction")
            elif ensemble_percent >= 50:    
                st.info("üîé Moderately Confident Prediction")
            else:
                st.warning("‚ö†Ô∏è Low Confidence ‚Äî Review borderline or unclear")    

            # BERT SHAP Explanation
            st.subheader("How It Interprets Your Review")
            try:
                shap_values = explain_with_shap(user_review, model, tokenizer)

                # Text plot
                st.write("**Which Words Mattered Most?**")
                shap_text_plot = shap.plots.text(shap_values[0], display=False)
                components.html(shap_text_plot, height=300, scrolling=True)

                # Waterfall plot for predicted class
                st.write("**See the Step-by-Step Decision Path**")
                plt.figure(figsize=(10,5))
                shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)
            except Exception as e:
                st.error(f"Error generating BERT SHAP explanation: {e}")

            # Metadata Analysis and Explanation
            st.subheader("Digging Deeper: Extra Clues in Your Review")
            try:
                features = extract_metadata_features(user_review)
                st.markdown("**Review Metadata Summary:**")
                for key, value in features.items():
                    st.markdown(f"- **{key.replace('_', ' ').title()}**: `{value}`")

                # Get combined input and separate components
                combined_input, text_vector, metadata_array = prepare_metadata_input(
                    features, vectorizer, user_review
                )
                
                # Scale input vector
                scaled_features = scaler.transform(combined_input)

                # Create background for KernelExplainer (use zeros with correct shape)
                if 'metadata_explainer' not in st.session_state:
                    background = np.zeros((1, scaled_features.shape[1]))
                    st.session_state.metadata_explainer = shap.KernelExplainer(metadata_model.predict_proba, background)

                # Calculate SHAP values
                shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
                expected_value = st.session_state.metadata_explainer.expected_value

                # Handle binary or multiclass output
                if isinstance(shap_values, list):
                    # shap_values is list of arrays for each class
                    shap_vals = shap_values[bert_pred][0]
                    base_val = expected_value[bert_pred]
                else:
                    # fallback
                    shap_vals = shap_values[0, :, bert_pred]
                    base_val = expected_value[bert_pred]

                # Get only the metadata feature names and their SHAP values
                num_text_features = text_vector.shape[1]  # Number of text features from vectorizer
                metadata_shap_vals = shap_vals[num_text_features:]  # Only take SHAP values for metadata features
                metadata_scaled_features = scaled_features[0, num_text_features:]  # Only scaled metadata features
                
                # Sorted metadata feature names
                metadata_feature_names = sorted(features.keys())

                # Create explanation object for plotting (metadata only)
                explanation = shap.Explanation(
                    values=metadata_shap_vals,
                    base_values=base_val,
                    data=metadata_scaled_features,
                    feature_names=metadata_feature_names
                )

                st.write("**Top Metadata Clues Behind the Prediction**")
                plt.figure(figsize=(10,5))
                shap.plots.bar(explanation, max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)

                st.write("**How Each Clue Pushed the Prediction**")
                plt.figure(figsize=(10,3))
                shap.plots.force(
                    base_val,
                    metadata_shap_vals,
                    features=metadata_scaled_features,
                    feature_names=metadata_feature_names,
                    matplotlib=True,
                    show=False
                )
                st.pyplot(plt.gcf(), clear_figure=True)

            except Exception as e:
                st.error(f"Error generating metadata explanation: {e}")

        if st.button("üîÑ Try Another Review"):
            st.session_state.user_review = ""
            st.experimental_rerun()

if __name__ == "__main__":
    main()
    """

import streamlit as st 
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model, scaler, vectorizer
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return metadata_model, scaler, vectorizer

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str) and text.strip():
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stop_words)
        features['sentiment'] = sia.polarity_scores(text)['compound']
    else:
        features = {key: 0 for key in [
            'review_length', 'uppercase_word_count', 'exclamation_count', 'question_count',
            'punctuation_count', 'avg_word_length', 'flesch_reading_ease', 'stopword_count', 'sentiment'
        ]}
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.cpu().numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        # x is a list of texts
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

def prepare_metadata_input(metadata_features, vectorizer, text):
    # Vectorize text (must be a list)
    text_vector = vectorizer.transform([text]).toarray()
    # Metadata features as array (shape: 1,9)
    metadata_array = np.array([[metadata_features[col] for col in sorted(metadata_features.keys())]])
    # Combine vectorized text and metadata features horizontally
    combined = np.hstack([text_vector, metadata_array])
    return combined, text_vector, metadata_array

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, vectorizer = load_metadata_model()

    if "reset" not in st.session_state:
        st.session_state.reset = False
    if st.session_state.reset:
        st.session_state.reset = False
        st.experimental_rerun()    

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
            return
        with st.spinner("Analyzing review and generating explanations..."):
            # BERT Prediction
            bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
            label = "üî¥ Fake Review " if bert_pred == 1 else "üü¢ Genuine Review "
            st.subheader(f"Prediction: {label}")

            features = extract_metadata_features(user_review)
            combined_input, text_vector, metadata_array = prepare_metadata_input(
                features, vectorizer, user_review
            )
            scaled_features = scaler.transform(combined_input)
            meta_probs = metadata_model.predict_proba(scaled_features)

            bert_prob_genuine = bert_probs[1]
            meta_prob_genuine = meta_probs[0][1]
            ensemble_prob_genuine = (bert_prob_genuine + meta_prob_genuine) / 2.0
            ensemble_pred = 1 if ensemble_prob_genuine > 0.5 else 0
            ensemble_percent = round(ensemble_prob_genuine * 100, 2)
            st.markdown(f"**üí° Confidence Behind the Answer:** {ensemble_percent}%")

            # Visual progress bar
            st.progress(int(ensemble_percent))

            if ensemble_percent >= 70:
                st.success("üåü Highly Confident Prediction")
            elif ensemble_percent >= 50:    
                st.info("üîé Moderately Confident Prediction")
            else:
                st.warning("‚ö†Ô∏è Low Confidence ‚Äî Review borderline or unclear")    

            # BERT SHAP Explanation
            st.subheader("How It Interprets Your Review")
            try:
                shap_values = explain_with_shap(user_review, model, tokenizer)

                # Text plot
                st.write("**Which Words Mattered Most?**")
                shap_text_plot = shap.plots.text(shap_values[0], display=False)
                components.html(shap_text_plot, height=300, scrolling=True)

                # Waterfall plot for predicted class
                st.write("**See the Step-by-Step Decision Path**")
                plt.figure(figsize=(10,5))
                shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)
            except Exception as e:
                st.error(f"Error generating BERT SHAP explanation: {e}")

            # Metadata Analysis and Explanation
            st.subheader("Digging Deeper: Extra Clues in Your Review")
            try:
                features = extract_metadata_features(user_review)
                st.markdown("**Review Metadata Summary:**")
                for key, value in features.items():
                    st.markdown(f"- **{key.replace('_', ' ').title()}**: {value}")

                # Get combined input and separate components
                combined_input, text_vector, metadata_array = prepare_metadata_input(
                    features, vectorizer, user_review
                )
                
                # Scale input vector
                scaled_features = scaler.transform(combined_input)

                # Create background for KernelExplainer (use zeros with correct shape)
                if 'metadata_explainer' not in st.session_state:
                    background = np.zeros((1, scaled_features.shape[1]))
                    st.session_state.metadata_explainer = shap.KernelExplainer(metadata_model.predict_proba, background)

                # Calculate SHAP values
                shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
                expected_value = st.session_state.metadata_explainer.expected_value

                # Handle binary or multiclass output
                if isinstance(shap_values, list):
                    # shap_values is list of arrays for each class
                    shap_vals = shap_values[bert_pred][0]
                    base_val = expected_value[bert_pred]
                else:
                    # fallback
                    shap_vals = shap_values[0, :, bert_pred]
                    base_val = expected_value[bert_pred]

                # Get only the metadata feature names and their SHAP values
                num_text_features = text_vector.shape[1]  # Number of text features from vectorizer
                metadata_shap_vals = shap_vals[num_text_features:]  # Only take SHAP values for metadata features
                metadata_scaled_features = scaled_features[0, num_text_features:]  # Only scaled metadata features
                
                # Sorted metadata feature names
                metadata_feature_names = sorted(features.keys())

                # Create explanation object for plotting (metadata only)
                explanation = shap.Explanation(
                    values=metadata_shap_vals,
                    base_values=base_val,
                    data=metadata_scaled_features,
                    feature_names=metadata_feature_names
                )

                st.write("**Top Metadata Clues Behind the Prediction**")
                plt.figure(figsize=(10,5))
                shap.plots.bar(explanation, max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)

                st.write("**How Each Clue Pushed the Prediction**")
                plt.figure(figsize=(10,3))
                shap.plots.force(
                    base_val,
                    metadata_shap_vals,
                    features=metadata_scaled_features,
                    feature_names=metadata_feature_names,
                    matplotlib=True,
                    show=False
                )
                st.pyplot(plt.gcf(), clear_figure=True)

            except Exception as e:
                st.error(f"Error generating metadata explanation: {e}")

        if st.button("üîÑ Try Another Review"):
            st.session_state.reset = True
            st.experimental_rerun()

if __name__ == "__main__":
    main()